{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASSIGNMENT: E-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a type of machine learning algorithm that is used for regression problems. It is based on the principle of boosting, which involves iteratively adding weak models to a base model in order to improve its accuracy. In Gradient Boosting Regression, the base model is a decision tree, and each iteration of the algorithm adds a new decision tree to the model.\n",
    "\n",
    "The algorithm works by first fitting a simple decision tree to the data. This tree is then used to predict the target variable, and the errors (or residuals) of the predictions are computed. A new decision tree is then fit to the errors, and its predictions are added to the predictions of the previous tree. This process is repeated iteratively, with each new decision tree fitted to the errors of the previous trees, until the model reaches a specified number of iterations or until the performance of the model stops improving.\n",
    "\n",
    "The key to Gradient Boosting Regression is the use of gradients to update the model at each iteration. The gradients represent the direction in which the model needs to be updated in order to improve its performance. By iteratively fitting decision trees to the residuals of the previous trees, the algorithm can gradually reduce the error of the model and improve its accuracy.\n",
    "\n",
    "Overall, Gradient Boosting Regression is a powerful machine learning algorithm that can be used to model complex relationships in data and achieve high predictive accuracy. It is widely used in a variety of fields, including finance, marketing, and healthcare, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a \n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's \n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('tips')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip     sex smoker  day    time  size\n",
       "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
       "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
       "2       21.01  3.50    Male     No  Sun  Dinner     3\n",
       "3       23.68  3.31    Male     No  Sun  Dinner     2\n",
       "4       24.59  3.61  Female     No  Sun  Dinner     4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 244 entries, 0 to 243\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype   \n",
      "---  ------      --------------  -----   \n",
      " 0   total_bill  244 non-null    float64 \n",
      " 1   tip         244 non-null    float64 \n",
      " 2   sex         244 non-null    category\n",
      " 3   smoker      244 non-null    category\n",
      " 4   day         244 non-null    category\n",
      " 5   time        244 non-null    category\n",
      " 6   size        244 non-null    int64   \n",
      "dtypes: category(4), float64(2), int64(1)\n",
      "memory usage: 7.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['sex'] = le.fit_transform(df['sex'])\n",
    "df['smoker'] = le.fit_transform(df['smoker'])\n",
    "df['time'] = le.fit_transform(df['time'])\n",
    "df['day'] = le.fit_transform(df['day'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tip  sex  smoker  day  time  size\n",
       "0  1.01    0       0    2     0     2\n",
       "1  1.66    1       0    2     0     3\n",
       "2  3.50    1       0    2     0     3\n",
       "3  3.31    1       0    2     0     2\n",
       "4  3.61    0       0    2     0     4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= df.drop('total_bill',axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    16.99\n",
       "1    10.34\n",
       "2    21.01\n",
       "3    23.68\n",
       "4    24.59\n",
       "Name: total_bill, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y= df['total_bill']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingRegressor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 44.01301305397787\n",
      "R-squared: 0.368140513551791\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to \n",
    "optimise the performance of the model. Use grid search or random search to find the best \n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 4, 6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50}\n",
      "Best cross-validation score:  0.5109068096101509\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search with cross-validation\n",
    "gbm = GradientBoostingRegressor()\n",
    "grid_search = GridSearchCV(gbm, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and corresponding scores\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set MSE:  36.60224769719268\n",
      "Test set R-squared:  0.47453092101568684\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_search.predict(X_test)\n",
    "print(\"Test set MSE: \", mean_squared_error(y_test, y_pred))\n",
    "print(\"Test set R-squared: \", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.  What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner is a model that is only slightly better than random guessing, and it typically has a high bias and low variance. In the context of Gradient Boosting, the weak learner is trained to predict the residuals of the previous model in the sequence, rather than the target variable itself. This means that each weak learner is designed to correct the mistakes of the previous model in the sequence.\n",
    "\n",
    "The most commonly used weak learner in Gradient Boosting is a decision tree with a small depth, also known as a decision stump. The decision stump is a tree with only one internal node and two leaf nodes. It splits the data based on a single feature and a threshold value and predicts a constant value in each leaf node. The decision stump is a simple model that can be trained quickly, and it is effective at capturing linear relationships between the input features and the target variable. However, it is not capable of capturing more complex relationships or interactions between the features, which is why it is typically used as a weak learner in Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be explained as follows:\n",
    "\n",
    "Suppose we have a regression or classification problem, and we want to predict the target variable Y based on a set of input features X. We start by fitting a simple model, such as a decision tree, to the data. However, this initial model is likely to have high bias and low variance, which means it may not capture the complexity of the relationship between X and Y very well.\n",
    "\n",
    "To improve the model, we need to reduce its bias and increase its variance. We do this by training a sequence of models, where each model is designed to correct the mistakes of the previous model. Specifically, we train a weak learner, such as a decision tree with small depth, on the residuals of the previous model. The residuals are the differences between the predicted values of the previous model and the true values of the target variable. By training the weak learner on the residuals, we can learn to capture the patterns that were missed by the previous model.\n",
    "\n",
    "We repeat this process, adding one model at a time to the sequence, until we reach a certain stopping criterion, such as a maximum number of models or a minimum improvement in performance. The final model is then the sum of all the individual models in the sequence.\n",
    "\n",
    "The key idea behind Gradient Boosting is to use the gradient of the loss function with respect to the predictions of the model as a guide for training the weak learners. By using the gradient, we can focus on the examples that are hard to predict and give them more weight in the training process. This allows us to improve the model's performance on these hard examples, which are typically the ones that have the most impact on the overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.  How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Gradient Boosting builds an ensemble of weak learners by training a sequence of models, where each model is designed to correct the mistakes of the previous model. Here's how the algorithm builds the ensemble:\n",
    "\n",
    "Initialize the ensemble with a single weak learner: The first model in the sequence is usually a simple model such as a decision tree with a small depth. This model is fit to the training data, and its predictions are used as the initial estimates for the target variable.\n",
    "\n",
    "Compute the residuals: The residuals are the differences between the predicted values of the current model and the true values of the target variable. The next weak learner is then trained on these residuals.\n",
    "\n",
    "Train the next weak learner: The next weak learner is designed to correct the mistakes of the previous model. It is trained on the residuals computed in step 2. The goal of this model is to learn the patterns in the data that were not captured by the previous model.\n",
    "\n",
    "Update the ensemble: The predictions of the new model are added to the predictions of the previous models in the ensemble. This sum is used as the new estimate for the target variable.\n",
    "\n",
    "Repeat steps 2-4: The process of computing residuals, training a new weak learner, and updating the ensemble is repeated multiple times. Each new model in the sequence is trained on the residuals of the previous models, and its predictions are added to the predictions of the previous models.\n",
    "\n",
    "Stopping criterion: The algorithm stops when a certain stopping criterion is met. This criterion can be a maximum number of models in the ensemble, a minimum improvement in performance, or any other user-specified condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting \n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical intuition behind Gradient Boosting can be constructed through the following steps:\n",
    "\n",
    "Define the Loss Function: The first step in constructing the mathematical intuition of Gradient Boosting is to define a loss function that measures the error between the predicted values and the true values of the target variable. The most common loss function used in regression problems is the Mean Squared Error (MSE), which is defined as the average of the squared differences between the predicted values and the true values.\n",
    "\n",
    "Initialize the model: The second step is to initialize the model with a constant value, which is the mean of the target variable. This model is used as the starting point for the optimization process.\n",
    "\n",
    "Calculate the Residuals: The third step is to calculate the residuals, which are the differences between the predicted values of the current model and the true values of the target variable. These residuals represent the errors of the current model.\n",
    "\n",
    "Train the Weak Learner: The fourth step is to train a weak learner, which is a simple model such as a decision tree, on the residuals calculated in step 3. The goal of this weak learner is to capture the patterns in the residuals that were not captured by the current model.\n",
    "\n",
    "Update the Model: The fifth step is to update the model by adding the predictions of the weak learner to the current model. This sum is used as the new estimate for the target variable.\n",
    "\n",
    "Repeat Steps 3-5: The process of calculating the residuals, training a weak learner, and updating the model is repeated multiple times until the residuals are minimized or a stopping criterion is met.\n",
    "\n",
    "Define the Learning Rate: The final step is to define a learning rate, which is a hyperparameter that controls the contribution of each weak learner to the final model. A smaller learning rate reduces the impact of each weak learner, while a larger learning rate increases it.\n",
    "\n",
    "By repeating steps 3-5 multiple times, the Gradient Boosting algorithm builds an ensemble of weak learners that collectively improve the accuracy of the predictions. The intuition behind the algorithm is that each weak learner is trained to correct the mistakes of the previous learner, and the model gradually becomes more accurate as more learners are added to the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
